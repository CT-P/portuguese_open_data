{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenizedDF= False\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "         \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "def create_tokenized_dataset(df_input):\n",
    "    # 1 create token column: tokens\n",
    "    df_input['tokens']=df_input['speech'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    # 2 create token without stopwords and stemmer: tokens_stemer_stop. 16min\n",
    "    df_input['tokens_stemer_stop'] = df_input['tokens'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))\n",
    "    \n",
    "    # 3 extract very frequent or rare words: token_cleaned\n",
    "    flat_tokens=[item for sublist in df_input['tokens_stemer_stop'] for item in sublist]\n",
    "    aux_c=Counter(flat_tokens)\n",
    "        \n",
    "    extrat_common=['par', 'nao', 'sr', 'deput', 'govern', 'muit', 'pel', 'president','tod','tamb','srs','sras','pod','part','psd','sao','aplaus','ja','porqu','faz','ha','diz','quer','pais','sobr','bem','nest']\n",
    "    extract_rare=[x[0] for x in aux_c.most_common()[-30:]]\n",
    "\n",
    "    df_input['tokens_cleaned'] = df_input['tokens_stemer_stop'].apply(lambda x : [i for i in x if not i in extrat_common])\n",
    "    df_input['tokens_cleaned'] = df_input['tokens_stemer_stop'].apply(lambda x : [i for i in x if not i in extract_rare])\n",
    "    df_input.to_pickel('df_input.pkl')\n",
    "    return df_input\n",
    "\n",
    "def generate_N_grams(text,ngram=1):\n",
    "  words=[word for word in text]  \n",
    "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "  ans=[' '.join(ngram) for ngram in temp]\n",
    "  return ans\n",
    "\n",
    "def create_grams(df_frame, n):\n",
    "    grams_d={1: 'uni_grams', 2: 'bi_grams', 3: 'tri_grams'}\n",
    "    df_frame[grams_d[n]] = df_frame['tokens_cleaned'].apply(lambda x: generate_N_grams(x,n))\n",
    "    return df_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tokenizedDF==True:\n",
    "    df_input = pd.read_pickle('parliament_fdf.pkl')  \n",
    "    df_tok=create_tokenized_dataset(df_input)\n",
    "else:\n",
    "    df_tok = pd.read_pickle('df_input.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok=create_grams(df_tok, 2)\n",
    "df_tok=create_grams(df_tok, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#political polarization functions\n",
    "\n",
    "right=[ 'PSD',  'CDS-PP', 'CH','IL','CDS']\n",
    "left=[ 'PS', 'BE', 'PCP', 'PAN', 'PEV','L']\n",
    "\n",
    "\n",
    "def create_frequency_table_grams(n_gram=1, indf=None, right_parties=right, left_parties=left):\n",
    "    grams_d={1: 'uni_grams', 2: 'bi_grams', 3: 'tri_grams'}\n",
    "   \n",
    "\n",
    "    r_grams=[item for sublist in indf[indf.party.isin(right)][grams_d[n_gram]] for item in sublist]\n",
    "    l_grams=[item for sublist in indf[indf.party.isin(left)][grams_d[n_gram]] for item in sublist]\n",
    "\n",
    "    total_counter = Counter([item for sublist in indf[grams_d[n_gram]] for item in sublist])\n",
    "    right_counter = Counter(r_grams)\n",
    "    left_counter = Counter(l_grams)\n",
    "\n",
    "    df_all = pd.DataFrame.from_dict(total_counter, orient='index').reset_index()\n",
    "    df_all.columns=['phrase','count']\n",
    "    df_all['f_right']=[right_counter[x] for x in df_all.phrase]\n",
    "    df_all['f_left']=[left_counter[x] for x in df_all.phrase]\n",
    "\n",
    "\n",
    "    df_all['f_left_total']=sum(left_counter.values())\n",
    "    df_all['f_right_total']=sum(right_counter.values())\n",
    "    df_all['f_right_minus']=(df_all['f_right']- df_all['f_right_total'])/df_all['f_right_total']\n",
    "    df_all['f_left_minus']=(df_all['f_left']- df_all['f_left_total'])/df_all['f_left_total']\n",
    "\n",
    "    df_all['f_right_norm']=df_all['f_right']/df_all['f_right_total']\n",
    "    df_all['f_left_norm']=df_all['f_left']/df_all['f_left_total']\n",
    "\n",
    "    df_all['f_right_minus_norm']=df_all['f_right_minus']/df_all['f_right_total']\n",
    "    df_all['f_left_minus_norm']=df_all['f_left_minus']/df_all['f_left_total']\n",
    "\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def calculate_pearson(df_all):\n",
    "    aa=df_all['f_right_norm']*df_all['f_left_minus_norm'] \n",
    "    bb=df_all['f_left_norm']*df_all['f_right_minus_norm']\n",
    "    cc=aa-bb\n",
    "    dd=cc*cc\n",
    "    d11=df_all['f_right_norm']+df_all['f_left_norm']\n",
    "    d22=df_all['f_right_norm']+df_all['f_right_minus_norm']\n",
    "    d33=df_all['f_left_norm']+df_all['f_left_minus_norm']\n",
    "    d44=df_all['f_right_minus_norm']+df_all['f_left_minus_norm']\n",
    "    denom=d11*d22*d33*d44\n",
    "    pp=dd/denom\n",
    "    return pp\n",
    "\n",
    "def create_phrase_partisanship(df):\n",
    "    aa=df['f_right_norm']+df['f_left_norm']\n",
    "    df['rho']=df['f_right_norm']/aa\n",
    "    bb=1-df['f_right_norm']\n",
    "    df['f_left_norm_scaled']=df['f_right_norm']/bb\n",
    "    cc=1-df['f_left_norm']\n",
    "    df['f_right_norm_scaled']=df['f_left_norm']/cc\n",
    "    df['pi']=df['f_right_norm']*df['rho']\n",
    "    df['pi_scaled']=(df['pi']/(1-df['f_right_norm']))+((1-df['pi'])/(1-df['f_left_norm']))\n",
    "\n",
    "    df['gram_partisanship']= 0.5 * (1 - df['pi_scaled'] + \n",
    "                           (df['f_right_norm_scaled'] +  df['f_left_norm_scaled']) * df['rho'])\n",
    "    return df\n",
    "\n",
    "def create_polarization_correlation(df):\n",
    "    \n",
    "    aa=df['f_left_norm']*-1\n",
    "    bb= df['f_right_norm']*1\n",
    "    df['beta_polarization']=aa+bb\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "         \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "def generate_N_grams(text,ngram=1):\n",
    "  words=[word for word in text]  \n",
    "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "  ans=[' '.join(ngram) for ngram in temp]\n",
    "  return ans\n",
    "\n",
    "def create_tokens(df_frame):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "    df_frame['tokenized_text'] = df_frame['speech1'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    df_frame['normalized_tokens'] = df_frame['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = None))\n",
    "    df_frame['normalized_tokens_count'] = df_frame['normalized_tokens'].apply(lambda x: len(x))\n",
    "    return df_frame\n",
    "\n",
    "def create_grams(df_frame, n):\n",
    "    df_frame['tri_grams'] = df_frame['normalized_tokens'].apply(lambda x: generate_N_grams(x,n))\n",
    "    return df_frame\n",
    "\n",
    "def apply_polarization_model(declaracoes2,right,left):\n",
    "    dfg3=create_frequency_table_grams(n_gram=3, indf=declaracoes2, right_parties=right, left_parties=left)\n",
    "    dfg3['pearson_quad']=calculate_pearson(dfg3)\n",
    "    trigrams_table=dfg3[dfg3.pearson_quad>0]\n",
    "    trigrams_table=create_phrase_partisanship(trigrams_table)\n",
    "    trigrams_table=create_polarization_correlation(trigrams_table)\n",
    "    final_df = trigrams_table.sort_values(by=['gram_partisanship'], ascending=False)\n",
    "    return final_df\n",
    "\n",
    "def create_200r(df_mainf):\n",
    "    indexes_no_applause=[n for n,x in enumerate(df_mainf.phrase) if 'aplausos' not in x]\n",
    "    df_mainf=df_mainf.iloc[indexes_no_applause]\n",
    "\n",
    "    tri_final = df_mainf[0:200].append(df_mainf[-200:], ignore_index=True)\n",
    "    return tri_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddb3f830e79c6f8e791b80246b7047706826e249abf27510d15251ed3cae5420"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
