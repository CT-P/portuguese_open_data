{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tokenizedDF= True\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "stop_words_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "snowball = nltk.stem.snowball.SnowballStemmer('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "         \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "def create_tokenized_dataset(df_input):\n",
    "    # 1 create token column: tokens\n",
    "    df_input['tokens']=df_input['speech'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    # 2 create token without stopwords and stemmer: tokens_stemer_stop. 16min\n",
    "    df_input['tokens_stemer_stop'] = df_input['tokens'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = snowball))\n",
    "    \n",
    "    # 3 extract very frequent or rare words: token_cleaned\n",
    "    flat_tokens=[item for sublist in df_input['tokens_stemer_stop'] for item in sublist]\n",
    "    aux_c=Counter(flat_tokens)\n",
    "        \n",
    "    extrat_common=['par', 'nao', 'sr', 'deput', 'govern', 'muit', 'pel', 'president','tod','tamb','srs','sras','pod','part','psd','sao','aplaus','ja','porqu','faz','ha','diz','quer','pais','sobr','bem','nest']\n",
    "    extract_rare=[x[0] for x in aux_c.most_common()[-30:]]\n",
    "\n",
    "    df_input['tokens_cleaned'] = df_input['tokens_stemer_stop'].apply(lambda x : [i for i in x if not i in extrat_common])\n",
    "    df_input['tokens_cleaned'] = df_input['tokens_stemer_stop'].apply(lambda x : [i for i in x if not i in extract_rare])\n",
    "    df_input.to_pickel('df_input.pkl')\n",
    "    return df_input\n",
    "\n",
    "def generate_N_grams(text,ngram=1):\n",
    "  words=[word for word in text]  \n",
    "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "  ans=[' '.join(ngram) for ngram in temp]\n",
    "  return ans\n",
    "\n",
    "def create_grams(df_frame, n):\n",
    "    grams_d={1: 'uni_grams', 2: 'bi_grams', 3: 'tri_grams'}\n",
    "    df_frame[grams_d[n]] = df_frame['tokens_cleaned'].apply(lambda x: generate_N_grams(x,n))\n",
    "    return df_frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_tokenizedDF==True:\n",
    "    df_input = pd.read_pickle('parliament_fdf.pkl')  \n",
    "    df_tok=create_tokenized_dataset(df_input)\n",
    "else:\n",
    "    df_tok = pd.read_pickle('df_input.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok=create_grams(df_tok, 2)\n",
    "df_tok=create_grams(df_tok, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>speech</th>\n",
       "      <th>filename</th>\n",
       "      <th>number</th>\n",
       "      <th>session</th>\n",
       "      <th>term</th>\n",
       "      <th>Date</th>\n",
       "      <th>link</th>\n",
       "      <th>party</th>\n",
       "      <th>speaker_ntime</th>\n",
       "      <th>genre</th>\n",
       "      <th>len_speech</th>\n",
       "      <th>year</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_stemer_stop</th>\n",
       "      <th>tokens_cleaned</th>\n",
       "      <th>bi_grams</th>\n",
       "      <th>tri_grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>josemanuelpureza</td>\n",
       "      <td>muito bem</td>\n",
       "      <td>darl13sl01n001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2015-10-23</td>\n",
       "      <td>https://debates.parlamento.pt/catalogo/r3/dar/...</td>\n",
       "      <td>BE</td>\n",
       "      <td>891.0</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>[muito, bem]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teresalealcoelho</td>\n",
       "      <td>muito bem</td>\n",
       "      <td>darl13sl01n001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2015-10-23</td>\n",
       "      <td>https://debates.parlamento.pt/catalogo/r3/dar/...</td>\n",
       "      <td>PSD</td>\n",
       "      <td>87.0</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>[muito, bem]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>luismontenegro</td>\n",
       "      <td>muito bom dia a todos sras e srs deputados cum...</td>\n",
       "      <td>darl13sl01n001.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2015-10-23</td>\n",
       "      <td>https://debates.parlamento.pt/catalogo/r3/dar/...</td>\n",
       "      <td>PSD</td>\n",
       "      <td>694.0</td>\n",
       "      <td>M</td>\n",
       "      <td>204</td>\n",
       "      <td>2015</td>\n",
       "      <td>[muito, bom, dia, a, todos, sras, e, srs, depu...</td>\n",
       "      <td>[bom, dia, sras, cumpr, prax, parlament, cabem...</td>\n",
       "      <td>[bom, dia, sras, cumpr, prax, parlament, cabem...</td>\n",
       "      <td>[bom dia, dia sras, sras cumpr, cumpr prax, pr...</td>\n",
       "      <td>[bom dia sras, dia sras cumpr, sras cumpr prax...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            speaker                                             speech  \\\n",
       "0  josemanuelpureza                                         muito bem    \n",
       "1  teresalealcoelho                                         muito bem    \n",
       "2    luismontenegro  muito bom dia a todos sras e srs deputados cum...   \n",
       "\n",
       "             filename  number  session  term        Date  \\\n",
       "0  darl13sl01n001.txt       1        1    13  2015-10-23   \n",
       "1  darl13sl01n001.txt       1        1    13  2015-10-23   \n",
       "2  darl13sl01n001.txt       1        1    13  2015-10-23   \n",
       "\n",
       "                                                link party  speaker_ntime  \\\n",
       "0  https://debates.parlamento.pt/catalogo/r3/dar/...    BE          891.0   \n",
       "1  https://debates.parlamento.pt/catalogo/r3/dar/...   PSD           87.0   \n",
       "2  https://debates.parlamento.pt/catalogo/r3/dar/...   PSD          694.0   \n",
       "\n",
       "  genre  len_speech  year                                             tokens  \\\n",
       "0     M           2  2015                                       [muito, bem]   \n",
       "1     F           2  2015                                       [muito, bem]   \n",
       "2     M         204  2015  [muito, bom, dia, a, todos, sras, e, srs, depu...   \n",
       "\n",
       "                                  tokens_stemer_stop  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [bom, dia, sras, cumpr, prax, parlament, cabem...   \n",
       "\n",
       "                                      tokens_cleaned  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [bom, dia, sras, cumpr, prax, parlament, cabem...   \n",
       "\n",
       "                                            bi_grams  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [bom dia, dia sras, sras cumpr, cumpr prax, pr...   \n",
       "\n",
       "                                           tri_grams  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [bom dia sras, dia sras cumpr, sras cumpr prax...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tok.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#political polarization functions\n",
    "\n",
    "right=[ 'PSD',  'CDS-PP', 'CH','IL','CDS']\n",
    "left=[ 'PS', 'BE', 'PCP', 'PAN', 'PEV','L']\n",
    "\n",
    "\n",
    "def create_frequency_table_grams(n_gram=1, indf=None, right_parties=right, left_parties=left):\n",
    "    grams_d={1: 'uni_grams', 2: 'bi_grams', 3: 'tri_grams'}\n",
    "   \n",
    "\n",
    "    r_grams=[item for sublist in indf[indf.party.isin(right)][grams_d[n_gram]] for item in sublist]\n",
    "    l_grams=[item for sublist in indf[indf.party.isin(left)][grams_d[n_gram]] for item in sublist]\n",
    "\n",
    "    total_counter = Counter([item for sublist in indf[grams_d[n_gram]] for item in sublist])\n",
    "    right_counter = Counter(r_grams)\n",
    "    left_counter = Counter(l_grams)\n",
    "\n",
    "    df_all = pd.DataFrame.from_dict(total_counter, orient='index').reset_index()\n",
    "    df_all.columns=['phrase','count']\n",
    "    df_all['count_right']=[right_counter[x] for x in df_all.phrase]\n",
    "    df_all['count_left']=[left_counter[x] for x in df_all.phrase]\n",
    "\n",
    "\n",
    "    df_all['count_left_total']=sum(left_counter.values())\n",
    "    df_all['count_right_total']=sum(right_counter.values())\n",
    "    # frequency of every words except the given one (by row)\n",
    "    #Jensen et al. (2012),p.10 \n",
    "    #f-pck is the frequency of all phrases used in Con­gress c by party k excluding phrase p\n",
    "    df_all['f_right_minus']=( df_all['count_right_total']-df_all['count_right'])/df_all['count_right_total']\n",
    "    df_all['f_left_minus']=( df_all['count_left_total']-df_all['count_left'])/df_all['count_left_total']\n",
    "\n",
    "    df_all['f_right']=df_all['count_right']/df_all['count_right_total']\n",
    "    df_all['f_right_norm'] = (df_all['f_right'] - df_all['f_right'].min()) / (df_all['f_right'].max() - df_all['f_right'].min())  \n",
    "    df_all['f_left']=df_all['count_left']/df_all['count_left_total']\n",
    "    df_all['f_left_norm'] = (df_all['f_left'] - df_all['f_left'].min()) / (df_all['f_left'].max() - df_all['f_left'].min())  \n",
    "\n",
    "    df_all['f_right_minus_norm']=(df_all['f_right_minus'] - df_all['f_right_minus'].min()) / (df_all['f_right_minus'].max() - df_all['f_right_minus'].min()) \n",
    "    df_all['f_left_minus_norm']=(df_all['f_left_minus'] - df_all['f_left_minus'].min()) / (df_all['f_left_minus'].max() - df_all['f_left_minus'].min()) \n",
    "\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def calculate_pearson(df_all):\n",
    "    aa=df_all['f_right_norm']*df_all['f_left_minus_norm'] \n",
    "    bb=df_all['f_left_norm']*df_all['f_right_minus_norm']\n",
    "    cc=aa-bb\n",
    "    dd=cc*cc\n",
    "    d11=df_all['f_right_norm']+df_all['f_left_norm']\n",
    "    d22=df_all['f_right_norm']+df_all['f_right_minus_norm']\n",
    "    d33=df_all['f_left_norm']+df_all['f_left_minus_norm']\n",
    "    d44=df_all['f_right_minus_norm']+df_all['f_left_minus_norm']\n",
    "    denom=d11*d22*d33*d44\n",
    "    pp=dd/denom\n",
    "    return pp\n",
    "\n",
    "def create_phrase_partisanship(df):\n",
    "    aa=df['f_right_norm']+df['f_left_norm']\n",
    "    df['rho']=df['f_right_norm']/aa\n",
    "    bb=1-df['f_right_norm']\n",
    "    df['f_left_norm_scaled']=df['f_right_norm']/bb\n",
    "    cc=1-df['f_left_norm']\n",
    "    df['f_right_norm_scaled']=df['f_left_norm']/cc\n",
    "    df['pi']=df['f_right_norm']*df['rho']\n",
    "    df['pi_scaled']=(df['pi']/(1-df['f_right_norm']))+((1-df['pi'])/(1-df['f_left_norm']))\n",
    "\n",
    "    df['gram_partisanship']= 0.5 * (1 - df['pi_scaled'] + \n",
    "                           (df['f_right_norm_scaled'] +  df['f_left_norm_scaled']) * df['rho'])\n",
    "    return df\n",
    "\n",
    "def create_polarization_correlation(df):\n",
    "    \n",
    "    aa=df['f_left_norm']*-1\n",
    "    bb= df['f_right_norm']*1\n",
    "    df['beta_polarization']=aa+bb\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normlizeTokens(tokenLst, stopwordLst = None, stemmer = None):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "\n",
    "    #Lowering the case and removing non-words\n",
    "    workingIter = (w.lower() for w in tokenLst if w.isalpha())\n",
    "\n",
    "    #Now we can use the semmer, if provided\n",
    "    if stemmer is not None:\n",
    "        workingIter = (stemmer.stem(w) for w in workingIter)\n",
    "         \n",
    "    #And remove the stopwords\n",
    "    if stopwordLst is not None:\n",
    "        workingIter = (w for w in workingIter if w not in stopwordLst)\n",
    "    #We will return a list with the stopwords removed\n",
    "    return list(workingIter)\n",
    "\n",
    "def generate_N_grams(text,ngram=1):\n",
    "  words=[word for word in text]  \n",
    "  temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "  ans=[' '.join(ngram) for ngram in temp]\n",
    "  return ans\n",
    "\n",
    "def create_tokens(df_frame):\n",
    "    nltk.download('stopwords')\n",
    "    stop_words_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "    df_frame['tokenized_text'] = df_frame['speech1'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    df_frame['normalized_tokens'] = df_frame['tokenized_text'].apply(lambda x: normlizeTokens(x, stopwordLst = stop_words_nltk, stemmer = None))\n",
    "    df_frame['normalized_tokens_count'] = df_frame['normalized_tokens'].apply(lambda x: len(x))\n",
    "    return df_frame\n",
    "\n",
    "def create_grams(df_frame, n):\n",
    "    df_frame['tri_grams'] = df_frame['normalized_tokens'].apply(lambda x: generate_N_grams(x,n))\n",
    "    return df_frame\n",
    "\n",
    "def apply_polarization_model(declaracoes2,right,left):\n",
    "    dfg3=create_frequency_table_grams(n_gram=3, indf=declaracoes2, right_parties=right, left_parties=left)\n",
    "    dfg3['pearson_quad']=calculate_pearson(dfg3)\n",
    "    trigrams_table=dfg3[dfg3.pearson_quad>0]\n",
    "    trigrams_table=create_phrase_partisanship(trigrams_table)\n",
    "    trigrams_table=create_polarization_correlation(trigrams_table)\n",
    "    final_df = trigrams_table.sort_values(by=['gram_partisanship'], ascending=False)\n",
    "    return final_df\n",
    "\n",
    "def create_200r(df_mainf):\n",
    "    indexes_no_applause=[n for n,x in enumerate(df_mainf.phrase) if 'aplausos' not in x]\n",
    "    df_mainf=df_mainf.iloc[indexes_no_applause]\n",
    "\n",
    "    tri_final = df_mainf[0:200].append(df_mainf[-200:], ignore_index=True)\n",
    "    return tri_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddb3f830e79c6f8e791b80246b7047706826e249abf27510d15251ed3cae5420"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
